"""
agent_based.py

æ”¹è¿›ç‰ˆ Materials-AI-Agent
------------------------

æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1) ä¸€æ¬¡æ€§æ‰§è¡Œ pipelineï¼šé€šè¿‡ --image_path/--work_root/--user_message å‚æ•°å¯åŠ¨
2) äº¤äº’å¼ CHATï¼šçœç•¥ --image_path åè¿›å…¥å¯¹è¯ï¼Œä½¿ç”¨ /run å‘½ä»¤è§¦å‘

ç”¨æ³•ç¤ºä¾‹ï¼š
  # ä¸€æ¬¡æ€§æ‰§è¡Œ
  python agent_based.py \
    --api_key YOUR_KEY \
    --image_path /path/to/img.png \
    --work_root ./results \
    --user_message "å…ƒç´ : Al,Sbï¼Œå‰‚é‡ 30k"

  # äº¤äº’å¼
  python agent_based.py --api_key YOUR_KEY
  User> /run /path/to/img.png ./results å…ƒç´ : Al,Sb

é€€å‡º interactive æ¨¡å¼è¾“å…¥ exit/quitã€‚
"""

import argparse
import json
import os
import re
import sys
import traceback
from pathlib import Path
from typing import Any, Dict

from openai import OpenAI  # pip install openai

# -----------------------------------------------------------------------
# å…¨å±€é»˜è®¤ï¼ˆå¯è¢« main è¦†ç›–ï¼‰
# -----------------------------------------------------------------------
_MODEL_NAME = "deepseek-chat"
_PIPELINE_DEFAULTS: Dict[str, str] = {
    "weight_path": "/home/aiprogram/project/yaotian/phase_structure_reconstruction/MOE_model_weights/moe_model.ckpt",
    "label_dir": "/home/aiprogram/project/yaotian/phase_structure_reconstruction/data_generation/label",
    "metadata_csv": "/home/aiprogram/project/yaotian/phase_structure_reconstruction/baseline/property.csv",
}

# -----------------------------------------------------------------------
# å¯¼å…¥ä¸»æµç¨‹å‡½æ•°ï¼ˆè¯·ç¡®ä¿ PYTHONPATH å·²åŒ…å«è¯¥æ¨¡å—ï¼‰
# -----------------------------------------------------------------------
try:
    from pipline_framework import run_agent_pipeline
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥ run_agent_pipelineï¼Œè¯·æ£€æŸ¥ PYTHONPATH å’Œæ¨¡å—åï¼")
    sys.exit(1)


# -----------------------------------------------------------------------
# å·¥å…·å‡½æ•°
# -----------------------------------------------------------------------
def run_structure_pipeline(
    image_path: str,
    work_root: str,
    user_message: str = "",
) -> str:
    """
    è°ƒç”¨å®Œæ•´ STEM â†’ é‡å»º â†’ shrink â†’ æ¾å¼›æµç¨‹ï¼Œè¿”å› JSON å­—ç¬¦ä¸²ã€‚
    """
    img_p = Path(image_path).expanduser().resolve(strict=True)
    root_p = Path(work_root).expanduser()
    root_p.mkdir(parents=True, exist_ok=True)

    try:
        result: Dict[str, Any] = run_agent_pipeline(
            image_path=str(img_p),
            user_message=user_message,
            work_root=str(root_p),
            weight_path=_PIPELINE_DEFAULTS["weight_path"],
            label_dir=_PIPELINE_DEFAULTS["label_dir"],
            metadata_csv=_PIPELINE_DEFAULTS["metadata_csv"],
        )
        return json.dumps(result, ensure_ascii=False)
    except Exception as exc:
        traceback.print_exc()
        return json.dumps({"error": str(exc)}, ensure_ascii=False)


# -----------------------------------------------------------------------
# OpenAI function schema
# -----------------------------------------------------------------------
_FUNCTIONS = [
    {
        "name": "run_structure_pipeline",
        "description": "ä¸Šä¼  STEM å¤§å›¾ï¼Œè‡ªåŠ¨é‡å»ºç»“æ„å¹¶æ¾å¼›ï¼Œè¿”å› JSON ç»“æœ",
        "parameters": {
            "type": "object",
            "properties": {
                "image_path": {"type": "string", "description": "æœ¬åœ° STEM å›¾åƒè·¯å¾„"},
                "work_root": {"type": "string", "description": "ç»“æœä¿å­˜ç›®å½•"},
                "user_message": {"type": "string", "description": "ç”¨æˆ·è¡¥å……è¯´æ˜"},
            },
            "required": ["image_path", "work_root"],
        },
    }
]
_TOOL_MAP = {"run_structure_pipeline": run_structure_pipeline}

# -----------------------------------------------------------------------
# ç³»ç»Ÿæç¤º
# -----------------------------------------------------------------------
_SYSTEM_PROMPT = (
    "ä½ æ˜¯ Materials-AI-Agentï¼Œèƒ½å¤Ÿå¯¹ STEM ç”µé•œå¤§å›¾è¿›è¡Œç»“æ„é‡å»ºä¸ç‰©æ€§åˆ†æã€‚\n"
    "1) è‹¥ç”¨æˆ·æä¾›å¤§å›¾åŠå·¥ä½œç›®å½•ï¼Œè¯·æ¨¡æ‹Ÿè°ƒç”¨ run_structure_pipelineï¼Œ"
    "å¹¶ä»¥ JSON æ ¼å¼è¿”å›è°ƒç”¨å‚æ•°ï¼š\n"
    "   {\"img\": \"<å›¾ç‰‡è·¯å¾„>\", \"work_root\": \"<å·¥ä½œç›®å½•>\", \"user_message\": \"<è¯´æ˜>\"}\n"
    "   ç„¶ååœ¨ä¸‹ä¸€è¡Œè¾“å‡ºç»“æ„åˆ†æç»“æœçš„ä¸“ä¸šä¸­æ–‡è§£è¯»ï¼ˆå«æ–‡ä»¶è·¯å¾„ã€èƒ½é‡ã€åº”åŠ›ç­‰ï¼‰ã€‚\n"
    "2) è‹¥ç”¨æˆ·åªæ˜¯ä¸€èˆ¬è¯¢é—®ï¼Œç›´æ¥å›å¤ä¸“ä¸šç­”æ¡ˆã€‚\n"
    "æ³¨æ„ï¼šJSON å—å¿…é¡»å¯è¢«ç¨‹åºç”¨æ­£åˆ™æå–ï¼Œä¸è¦æœ‰é¢å¤–æ–‡å­—å¹²æ‰°ã€‚"
)


# -----------------------------------------------------------------------
# äº¤äº’é€»è¾‘
# -----------------------------------------------------------------------
def chat(args) -> None:
    # ---------- DeepSeek Chat ----------
    client = OpenAI(api_key=args.api_key, base_url=args.api_base)
    messages = [{"role": "system", "content": _SYSTEM_PROMPT}]
    print("ğŸ”¹ Materials-AI-Agent å¯åŠ¨ï¼Œè¾“å…¥ exit/quit é€€å‡ºã€‚\n")
    print("ğŸŸ¢ æç¤ºï¼šåœ¨äº¤äº’ä¸­å¯ä½¿ç”¨ `/run <å›¾ç‰‡è·¯å¾„> <å·¥ä½œç›®å½•> [è¯´æ˜]` ç›´æ¥è§¦å‘ç»“æ„åˆ†æã€‚\n")

    pipeline_executed = False

    while True:
        user_in = input("User> ").strip()
        if user_in.lower() in {"exit", "quit"}:
            break
        if not user_in:
            continue

        # ä¸€æ¬¡æ€§ /run å‘½ä»¤æ€»æ˜¯è§¦å‘æµæ°´çº¿
        if user_in.startswith("/run"):
            pipeline_executed = True
            parts = user_in.split(maxsplit=4)
            if len(parts) < 3:
                print("â— æ ¼å¼ï¼š/run <å›¾ç‰‡è·¯å¾„> <å·¥ä½œç›®å½•> [å¯é€‰è¯´æ˜]")
                continue
            _, img, wd, *rest = parts
            umsg = rest[0] if rest else ""
            print(f"ğŸ”§ ç«‹å³æ‰§è¡Œ pipeline: img={img}, work_root={wd}")
            out = run_structure_pipeline(img, wd, umsg)
            print("ğŸ“ Pipeline è¾“å‡ºï¼š", out)
            # å›å¡«çœŸå®ç»“æœå¹¶è®©æ¨¡å‹è§£è¯»
            messages.append({"role": "tool", "name": "run_structure_pipeline", "content": out})
            resp = client.chat.completions.create(
                model=_MODEL_NAME,
                messages=messages,
                temperature=0.2,
            )
            reply = resp.choices[0].message.content
            print(f"\nAssistant> {reply}\n")
            messages.append({"role": "assistant", "content": reply})
            continue

        # 2) æ™®é€šå¯¹è¯ï¼Œä½†å¦‚æœå·²ç»è·‘è¿‡æµæ°´çº¿ï¼Œå°±ç»ä¸å†è§¦å‘å®ƒ
        if pipeline_executed:
            # åªèµ°ã€Œæ€»ç»“/æ•´ç†ã€åˆ†æ”¯
            messages.append({"role": "user", "content": user_in})
            resp = client.chat.completions.create(
                model=_MODEL_NAME,
                messages=messages,
                temperature=0.2,
            )
            print("\nAssistant>", resp.choices[0].message.content, "\n")
            messages.append(resp.choices[0].message)
            continue

        # æ™®é€šå¯¹è¯åˆ†æ”¯ï¼šè¯¢é—®æ¨¡å‹æ˜¯å¦éœ€è¦è°ƒç”¨æµç¨‹
        user_prompt = (
            "è¯·åˆ¤æ–­æ˜¯å¦éœ€è°ƒç”¨ç»“æ„åˆ†ææµç¨‹ï¼ˆrun_structure_pipelineï¼‰ï¼Œ"
            "è‹¥éœ€è¦ï¼Œè¯·é¦–å…ˆè¾“å‡ºä¸€ä¸ªçº¯ JSON å¯¹è±¡ï¼ŒåŒ…å«é”® img, work_root, user_messageï¼›\n"
            f"ç„¶ååœ¨ä¸‹ä¸€è¡Œå¼€å§‹è¯·ç”¨æˆ·è€å¿ƒç­‰å¾…ï¼Œå¹¶ä¸”ç»™å‡ºå¤§è‡´æ‰€éœ€çš„æ—¶é—´ï¼š\n{user_in}"
        )
        messages.append({"role": "user", "content": user_prompt})

        resp = client.chat.completions.create(
            model=_MODEL_NAME,
            messages=messages,
            temperature=0.2,
        )
        content = resp.choices[0].message.content
        print(f"\nAssistant> {content}\n")
        messages.append(resp.choices[0].message)

        # å°è¯•ä»å›å¤ä¸­æå– JSON å¹¶è§¦å‘çœŸå®è°ƒç”¨
        try:
            json_block = re.search(r"\{[\s\S]*?\}", content).group(0)
            params = json.loads(json_block)
            img = params["img"]
            wd = params["work_root"]
            umsg = params.get("user_message", "")
            print(f"ğŸ”§ Detected trigger, è°ƒç”¨ run_structure_pipeline(img={img}, work_root={wd}, user_message={umsg})")
            real_out = run_structure_pipeline(img, wd, umsg)
            pipeline_executed = True
            print("ğŸ“ å®é™… pipeline è¿”å›ï¼š", real_out)
            # å†æ¬¡è®©æ¨¡å‹ç”¨çœŸå®ç»“æœç”Ÿæˆè§£è¯»
            messages2 = messages + [
                {
                    "role": "assistant",                  # æ–°ç‰ˆ SDK è¦ä¹ˆç”¨ "tool"ï¼Œè¦ä¹ˆç”¨ "assistant"
                    "name": "run_structure_pipeline",
                    "content": real_out,
                    }
                ]
            resp2 = client.chat.completions.create(
                model=_MODEL_NAME,
                messages=messages2,
                temperature=0.2,
            )
            followup = resp2.choices[0].message.content
            print(f"\nAssistant> {followup}\n")
            messages.append(resp2.choices[0].message)
        except Exception:
            # æ—  JSON æˆ–è§£æå¤±è´¥ï¼Œåˆ™ç›´æ¥å½“ä½œæ™®é€šé—®ç­”
            pass


# -----------------------------------------------------------------------
# Main: å‚æ•° & æ¨¡å¼åˆ‡æ¢
# -----------------------------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Materials-AI-Agent â€” ä¸€æ¬¡æ€§æˆ–äº¤äº’å¼æ‰§è¡Œ STEM æµæ°´çº¿"
    )
    parser.add_argument("--api_key",     default=None)
    parser.add_argument("--api_base",    default="https://api.deepseek.com")
    parser.add_argument("--model_name",  default=_MODEL_NAME)
    parser.add_argument("--weight_path", default=_PIPELINE_DEFAULTS["weight_path"])
    parser.add_argument("--label_dir",   default=_PIPELINE_DEFAULTS["label_dir"])
    parser.add_argument("--metadata_csv",default=_PIPELINE_DEFAULTS["metadata_csv"])
    parser.add_argument("--image_path",  default=None, help="è‹¥æŒ‡å®šï¼Œåˆ™è„šæœ¬å¯åŠ¨åç›´æ¥æ‰§è¡Œä¸€æ¬¡æ€§ pipeline")
    parser.add_argument("--work_root",   default='/home/aiprogram/project/yaotian/phase_structure_reconstruction/baseline', help="ä¸€æ¬¡æ€§ pipeline çš„ç»“æœä¿å­˜ç›®å½•")
    parser.add_argument("--user_message", default=None, help="ä¸€æ¬¡æ€§ pipeline çš„ç”¨æˆ·è¡¥å……è¯´æ˜")

    args = parser.parse_args()
    # è¦†ç›–é…ç½®
    _MODEL_NAME      = args.model_name
    _PIPELINE_DEFAULTS["weight_path"]  = args.weight_path
    _PIPELINE_DEFAULTS["label_dir"]    = args.label_dir
    _PIPELINE_DEFAULTS["metadata_csv"] = args.metadata_csv

    # ä¸€æ¬¡æ€§æ¨¡å¼
    if args.image_path and args.work_root:
        print("ğŸ”¹ ä¸€æ¬¡æ€§æ¨¡å¼ï¼šæ‰§è¡Œ run_structure_pipeline ...")
        out = run_structure_pipeline(args.image_path, args.work_root, args.user_message)
        print("\nğŸ”¹ JSON ç»“æœï¼š")
        print(out)
        # å°† out ä¸ç”¨æˆ·è¡¥å……è¯´æ˜ä¸€èµ·å‘é€ç»™æ¨¡å‹è¿›è¡Œè§£è¯»
        messages = [
            {"role": "system", "content": _SYSTEM_PROMPT},
            {"role": "user", "content": args.user_message or "è¯·è§£æä»¥ä¸‹ç»“æœå¹¶ç»™å‡ºä¸“ä¸šè§£è¯»ã€‚"},
            {"role": "assistant", "name": "run_structure_pipeline", "content": out},
        ]
        client = OpenAI(api_key=args.api_key, base_url=args.api_base)

        resp = client.chat.completions.create(
            model=_MODEL_NAME,
            messages=messages,
            temperature=0.2,
        )
        reply = resp.choices[0].message.content
        print(f"\nAssistant> {reply}\n")
        sys.exit(0)

    # äº¤äº’å¼æ¨¡å¼
    chat(args)
